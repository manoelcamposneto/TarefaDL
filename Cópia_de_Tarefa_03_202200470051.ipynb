{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9auw6rOhspmk5Ipou5JEP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manoelcamposneto/TarefaDL_03/blob/main/C%C3%B3pia_de_Tarefa_03_202200470051.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AMOSTRA 1\n",
        "![AMOSTRA 1.png](data:image/png;base64,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)\n"
      ],
      "metadata": {
        "id": "O0iuSY1quSda"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSJqtUKOFFOz"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras import backend as k\n",
        "from keras import losses\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Initialize all weights to match the values adopted in the example at\n",
        "#https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ \n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2) \n",
        "    # and create a fixed array with this dimension for layer 1\n",
        "    kernel = np.array([[0.1, 0.3],[-0.4, 0.2]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2)\n",
        "    # and create a fixed array with this dimension for layer 2\n",
        "    kernel = np.array([[0.4, -0.4], [0.2, 0.7]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 1\n",
        "    kernel = np.array([-0.4, 0.8]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 2\n",
        "    kernel = np.array([0.80, 0.2]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "# from https://stackoverflow.com/questions/66221788/tf-gradients-is-not-supported-when-eager-execution-is-enabled-use-tf-gradientta\n",
        "# TF 2 does not use \"eager\" execution, so disable it:\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "#Define the neural network model with dense layers. Syntax:\n",
        "#https://keras.io/api/layers/core_layers/dense/\n",
        "#The sigmoid activation function in Keras is the standard logistic function 1/(1+exp(-x)).\n",
        "#https://keras.io/api/layers/activations/\n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=2, use_bias=True,  bias_initializer=kernel_init_bias1,\n",
        "        kernel_initializer=kernel_init_layer1, activation='sigmoid'))\n",
        "model.add(Dense(2, use_bias=True,  bias_initializer=kernel_init_bias2, \n",
        "        kernel_initializer=kernel_init_layer2, activation='sigmoid'))\n",
        "model.summary() # display the architecture\n",
        "\n",
        "# We are not going to use a Keras optimizer. Define a learning rate:\n",
        "learning_rate = 1  #you can change to 0.5 or any other reasonable value\n",
        "\n",
        "#now compile the model, informing loss and performanc metrics that\n",
        "#should be computed along the training\n",
        "model.compile(loss='mse', metrics=['accuracy'])\n",
        "\n",
        "# Begin TensorFlow\n",
        "sess = tf.compat.v1.InteractiveSession()\n",
        "sess.run(tf.compat.v1.initialize_all_variables())\n",
        "\n",
        "print(\"===START TRAINING THE NETWORK:===\")\n",
        "steps = 3 # steps of gradient descent\n",
        "\n",
        "model.optimizer.lr = learning_rate\n",
        "\n",
        "for s in range(steps):\n",
        "    print('Learning rate = ', k.get_value(model.optimizer.lr))\n",
        "    print(\" ############ Step = \" + str(s) + \" ############\")\n",
        "    print(\"1) ===BEFORE using any GRADIENT:===\")\n",
        "    #define input and target vectors, and also inform it to the loss function object\n",
        "    #in this code we will keep the same inputs and targets along the steps, but the inputs and\n",
        "    #targets can change when we go, for instance, through the training set\n",
        "    inputs = np.array([[0.0, -1]]) #define the input for the current iteration (step)\n",
        "    outputs = model.predict(inputs) #forward pass\n",
        "    #define the target vector for the current iteration (step)\n",
        "    targets = np.array([[0, -1.3]]) #notice that a sigmoid can output within range [0, 1]\n",
        "    mse = mean_squared_error(targets, outputs) #calculate MSE\n",
        "    #initialize loss object to be later incorporated to the gradients object that\n",
        "    #enables the calculation of the symbolic gradients\n",
        "    loss = losses.mean_squared_error(targets, model.output)\n",
        "    #  ===== Obtain symbolic gradient to calculate numerical gradients =====\n",
        "    gradients = k.gradients(loss, model.trainable_weights) #inform loss and weights\n",
        "    if False: #enable with True in case you want to see the objects\n",
        "        print(\"List of tensors representing the symbolic gradients:\")\n",
        "        for i in range(len(gradients)):\n",
        "            print('symbolic gradient[',i,']=',gradients[i])\n",
        "    print('Network input [x1, x2]:\\n', inputs)\n",
        "    print('Network output [out_o1, out_o2]:\\n', outputs)\n",
        "    print(\"targets:\\n\", targets)\n",
        "    #show weights at the beginning of iteration s\n",
        "    print('weights at the beginning of this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        #note that model.trainable_weights[i] is an object of\n",
        "        # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "        #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "        #you need to obtain its value via a TF session:\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "    print(\"MSE with the initial weights:\", mse)\n",
        "\n",
        "    print(\"2) ===After applying GRADIENT:===\")\n",
        "    print(\"------------- Results for step (iteration) =\", s)\n",
        "    # ===== Calculate numerical gradient from symbolic gradients =====\n",
        "    # evaluated_gradients is a list, show its contents:\n",
        "    evaluated_gradients = sess.run(gradients, feed_dict={model.input: inputs})\n",
        "    print('Gradients g to be used in new_weights = current_weights - learning_rate*g')\n",
        "    for i in range(len(evaluated_gradients)):\n",
        "        print('gradients[',i,']=',evaluated_gradients[i])\n",
        "\n",
        "    # Apply (\"step down\") the gradient for each layer, subtracting the gradients\n",
        "    # from current weights scaled by the learning rate:\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        sess.run(tf.compat.v1.assign_sub(model.trainable_weights[i], learning_rate*evaluated_gradients[i]))\n",
        "\n",
        "    #show weights after gradient propagation of iteration s\n",
        "    print('weights after gradient propagation in this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "\n",
        "    # print the MSE with new weights:\n",
        "    outputs = model.predict(inputs)\n",
        "    mse = mean_squared_error(targets, outputs)\n",
        "    print(\"MSE with the new weights:\", mse)\n",
        "\n",
        "#Collect and show final results\n",
        "final_outputs = model.predict(inputs)\n",
        "final_mse = mean_squared_error(targets, final_outputs)\n",
        "\n",
        "print(\"\\n ===AFTER executing all GRADIENT descent steps===\")\n",
        "#show weights at the end of training\n",
        "for i in range(len(model.trainable_weights)):\n",
        "    #note that model.trainable_weights[i] is an object of\n",
        "    # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "    #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "    #you need to obtain its value via a TF session:\n",
        "    print('final weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "print(\"outputs:\\n\", final_outputs)\n",
        "print(\"targets:\\n\", targets)\n",
        "print(\"Final MSE = \", final_mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AMOSTRA 2\n",
        "![AMOSTRA 2.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOsAAABgCAIAAABzHxKZAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAA0eSURBVHhe7Z3fTxRXG8eH914M4pUR04A3RhONoLFCTDQVURNTo83ij6uagmuMFrVgENvaoAlbldZERS2Nd2AD8QqUpPFGaxXBQKrpjXBBE69EUP8A3u/M83Acd37sLOzO7IHnk+30mTNnd53nfPc5zzkzzMmbmpoyBEFb/sf/FwQ9EQULeiMKFvRGFCzojShY0BtRsKA3omBBb0TBgt6IggW90VjB1dXVeTa4NHQWLVrE/4K8vHXr1nHpPMB+4lVVVVyaIeyNiy8aHR3lAw40VvDk5CS2JSUl2yyoMHwqKyvx7WVlZbAHBgaocD4wMTGBLU4cp19aWkqFmWLt2rVWq5rNii96/fo1lbsw5QUO4VVezrtJnDzJFaKDTq+lpYX33RgZGaFq2HJRMN68edPW1oZ3FRQUWH4ymwolfNjBw4cPqRrvzxLl3q4uLrEzNjaVn28eRbXooPPFifN+msCZ5Fv/T0j5Ld4eV05sb+cSxeAgH6qp4ZIoSKlgHFL6S0vB0L16YxLQMcTN9WxkWMFKoytXcomdWMw8hApoiOig852BgoeGhqjtiFkq2DuLOH7cKCoyjR9/NP77zyqa5swZc5ufbzQ1Wfs5x/DwMFLShoYG6unSBX0Wci/8AOBrchM8qPKE9vZ2qpZF4PmTJ03j5Uvjp5+somn+/NO4c8c0vvkGfa1VpBOJRGLNmjV9fX28P3uohdxBF4YKeNl7K1XY2solEeETgymCQnNQHlVLKwa7gsBs+svKvLnIBr6IjvJ+RkAKhw9ErEVIVlBhURHvRgedr38ETQKZA70LrdbT00O2/yekrOM7ktuzx6Ax5q1bxvPnVpFh1NWZ2/Jy49tvrf1cBCLr6Oh49uxZRUUFF82a4uJi+jEoKWcd9H7g/XujudnaN4xffjH++ss0Wlutff2Ix+NwYH19fT768EyQai7i/HkzW4ATKXNAj0YZxZUr5jZXgXarq6t5R1+++MKoqTGNmzfN5AGev3zZ3I3FzOCiIbW1tdeuXUMs4P1MkErByLSQb4H7943ffzcuXTJtpGheGRhiXl6eGSp86ezs5Lk+b5DLcu2cob+/H1v7KCTrYKRBsQrx+NdfTRFj9+efrWM20EPC7eqVyv9HjhxhL3uAYQBXzXkCzAdfvMhDukOHzGAMG4M8J/AafEd93Fykt7eXxoW7d++mkjCAt8+dMw04VoUPag47X35pdHUhBzdfiNDI9FKJeM4QQMHAnnWdOOHiQcQAeA2+gx8DgC6e83BvVq9ezbVzgPHx8WPHjsFAhr13714qDAmMNzDqIOD5779n287Y2Me8orPT3Po2BLpy9rIHb9++5ao5TzAF//03G+Dff9mwg6QCv37y3Vzk7NmzNIC7fft2YWEhFYYEMod//mH73buPQ2ofnCFm7hJAwXDZrVumQX6hUcV8Aln79evXYbS0tGRwciMozc1m8ob01z6k9geiX7qU7XBJulmF8LmrYfYEUDBcRukvUjESMc3yzA8g33379sGIx+P19fVUGB4IFggZANkwXePAkLq72zS8OHXK3H71lbUz90mlYDgLLgNQLeSLJBhAyrMbKOgyF2GXL3JHKgwVChYrV5rZMDJgiiA0Je8KOkwM+FJNt2VvLgIe41TaRmanz5JIpWB1/eLrr01DjSp++IEnhucujx49ili+CBM0t0PTEYCG1PA8BVonpaWmyufugMSJr4LhJpKp/foFaRp5hbpQlD65PxeBHmDXrl0wYrFYNPKF5xEmgD2gwqAIYr9Kqli2zNyOjVk7fsyPuQjqj0BNzSfXL+BE+BTMiSEdBhl0p/aNGze4yJLv5s2bJyYmysrKrl69yqUh8913PIBLSr4pmjiHdBhiQvSDg7w7f+AfnRPX20qIwUG+98959zDd9xPKTT8+d/aUlJTQ2blif4u6I8d+6w/SBir0AoGZq06T4Tt7XO+pUtTU8FF19zDdb6l2Q4HON607ezo6OuhdXnA9G1Se/p09CK4qA6PRgx11qRl1aFyMgE3XM2m2H5kGbOrUoiB4J7hkyRK6kc1+pe2zzz5jKyp++83cYgDnevlTXWqmtBjOp/st4XxqBXrl3mW5d+/esZVBWMka4hODwyfDMVgH6HzTisEzIOW3pJqLEITcRhQs6I3GT8Cuqqrq6+vDoG358uXYvU9XXkKnurp6cnJyfHyc/lBZX3+mSx5SbesPYQoLC0tLS8+fP0/lGSGRSDx48AAG/T0SsgjP6/lWLqElMZrUm8b1DzBDwP43oWhOLp0HZPXE7Y2LLxoZGeEDDmQVAkFvJA8W9EYULOiNKFjQG1GwoDeiYEFvRMGC3oiCBb0RBQt6IwoW9EYULOiNKFjQG1GwoDeiYEFvRMGC3oiCBb0RBQt6IwoW9EYULOiNKFjQG1GwoDeiYEFvRMGC3oiCBb0RBQt6o6uCb9y4YS34wDx69IgPBGZ4eJjfbHEmyBpBOQk9wZuoonWwQydCZ+qqYHoSbUFBwTaLBQsWUHlw8BZ6Lz0ue1Dbp5/TSqNlZWU4l9LSUioMmSidSQ+fSubcuSkcys83H9fuZGyMn+FeU8MlodPS0oJ/PFzG+58yMjISj8fVk9xhYNfr2Vv+H8XksEPoHL2esIvyWCymnnEGoeN803rGnOsn8DEHgZyZUTwUrJrE8bh9E2pOvFybMxR8PDU0NGR/KJ0ChTjElWwEcnoOO4TOzlXBXk/9hwoDithrQQavTwhfwR5ZRFERrzNw547Lmjn0jPxYzHOF++gYHx9Xi7ioRkVDQr4oxCFU4KppoaFD1GpiiKDU/0BzpLCBgYEDBw5YtfxIJBK0mKnqwbBVn3D58mWrVtRYTeyGV9RpbY0w3ii8fuuNjY0oh16TIgTUbJ2u0dbWxkXTBA0bueoQOi9nDMZv2PW86HyBa49kh7oy/AB4fxoVmJ1hOKgzM4f3SM4r6tAvLycDMFAxI2n97oqKCmrRu3fvUknaaOWQ4eFheiI3Lclv59ChQ2TQw6W9QAinMeLRo0epRHFqej3Gp0+fkhEhvnMRx4/zmjmJhLVvrTJJaySGv8JwANBs5PTt27dTiZ3Kykps/ZstBfo45MmTJ2Ts2LGDDAV+2/Rjpoekz4Di4mIKzy9evKCSCPFVsDPq+Mebioogi0CNjo7ytKE39vUJg/PhwwcylixZQoadhQsXsjVjgjsEUUqtimU9rd+V7K1vTLONajYmiaQOagasX78e2+fOIUHopJoPVlEHkvKJNziEdqIl6KLj8ePHZLguRb1x40YyZnD54yNBHNLdbfzxB0YY/ALeIs4SFF9phREnW7Zswda/O1JT7MqrTiYnJ9mKjlQKVlGns9Po6jKNpEVqAX6IdXVmHKIKqYC8OAn3pra2lmvnGkEcsmfPJ4sb07qFbl1TLq9vvHr1agrhDQ0N6BJpDgfb3t5edB2zSsYySioFA0QdNNv792aIRfhpauJyBdoPkWb+rKee0iFJoLKedHd3U757+PDhxYsXI6vBdufOnTRczhECKBgNcOIE2wg/2raHYgaXoD8hXYdQppF7fiN1+oAwPDAwEHdc3Xz48OE2a0HVqC5i2wmgYKC8//nnbOQky6ZXch4eHibDzsuXL8lAw5Axc9JyCEZ7qI/UIkRoJNDf30+7SdAIjEZj/uBzkOq8evWKshoY2K2oqICBo8rhERJMwRkle3MRS5cuJUNNStgZs3JTr+F5tjh1yozBlAo7yN5cBC1uTnOLTkZGRrB1He8GAS1In7BhwwYqiZAIFJw9VqxYQca9e/fIsHPnjrkAPM0KhwRGb5cuGSdPhhyAwapVq8jAwIsMBfRHFzs2bdpEJenSZQ1hEQsy0JvNmggUnL25iMLCQloLEkONpPsf0JAUNvbv308lWae7m6doLl7kEgfZm4vYsWMHpblXrlyhEkV7ezu2OLp161YqSQt4sqGhAUZzczOVRMucisGALoGi96yqqqJsGFLu7Ow8ePAgbIw/PJfnzSxINPfuNcrLI5yiOX36NLZ9fX3IVRB3YWObSCQuXLhAR+3XNaBLSlrsk+WoDNfRewH8iZKdO3fChierq6upPGL4x+5PVxffvALDB6rW2sq72cTnDpK2tjY6tSQyeUNgSofgUFER21mGzs717krqkZzE43GuMY263QcGF01N0YSDE3ysqydBLt3ZY0ddj3W9MIt4Q9dOEXUAuk7Y0Y1SkYGgOeFlNVsE7ULWz549m/3VVMbfIXTjCwZw5Bb1Ch1E0I6ODrsQ4Zaenh5kL7w/DeXN8Ji6cgmamppQ3zmVho/NmCdnDytZNzL4Ww8/bGQWqxk9/0YjZHI1BgtCriIKFvRGbwX39/dXWbhehPMHQ2x6782bN7lIZ+rq6nAuUT0zIEpncjahG0kTDhid8IHADA0N8ZstGhsb+YBu2G9vwJiVS8MlQmfm4T/+WkHQEMmDBb0RBQt6IwoW9EYULOiNKFjQG1GwoDeiYEFvRMGC3oiCBb0RBQs6Yxj/B3Ra2Z8qItzpAAAAAElFTkSuQmCC)"
      ],
      "metadata": {
        "id": "OhKkClRVu7am"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras import backend as k\n",
        "from keras import losses\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Initialize all weights to match the values adopted in the example at\n",
        "#https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ \n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2) \n",
        "    # and create a fixed array with this dimension for layer 1\n",
        "    kernel = np.array([[0.1, 0.3],[-0.4, 0.2]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2)\n",
        "    # and create a fixed array with this dimension for layer 2\n",
        "    kernel = np.array([[0.4, -0.4], [0.2, 0.7]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 1\n",
        "    kernel = np.array([-0.4, 0.8]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 2\n",
        "    kernel = np.array([0.80, 0.2]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "# from https://stackoverflow.com/questions/66221788/tf-gradients-is-not-supported-when-eager-execution-is-enabled-use-tf-gradientta\n",
        "# TF 2 does not use \"eager\" execution, so disable it:\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "#Define the neural network model with dense layers. Syntax:\n",
        "#https://keras.io/api/layers/core_layers/dense/\n",
        "#The sigmoid activation function in Keras is the standard logistic function 1/(1+exp(-x)).\n",
        "#https://keras.io/api/layers/activations/\n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=2, use_bias=True,  bias_initializer=kernel_init_bias1,\n",
        "        kernel_initializer=kernel_init_layer1, activation='sigmoid'))\n",
        "model.add(Dense(2, use_bias=True,  bias_initializer=kernel_init_bias2, \n",
        "        kernel_initializer=kernel_init_layer2, activation='sigmoid'))\n",
        "model.summary() # display the architecture\n",
        "\n",
        "# We are not going to use a Keras optimizer. Define a learning rate:\n",
        "learning_rate = 1  #you can change to 0.5 or any other reasonable value\n",
        "\n",
        "#now compile the model, informing loss and performanc metrics that\n",
        "#should be computed along the training\n",
        "model.compile(loss='mse', metrics=['accuracy'])\n",
        "\n",
        "# Begin TensorFlow\n",
        "sess = tf.compat.v1.InteractiveSession()\n",
        "sess.run(tf.compat.v1.initialize_all_variables())\n",
        "\n",
        "print(\"===START TRAINING THE NETWORK:===\")\n",
        "steps = 3 # steps of gradient descent\n",
        "\n",
        "model.optimizer.lr = learning_rate\n",
        "\n",
        "for s in range(steps):\n",
        "    print('Learning rate = ', k.get_value(model.optimizer.lr))\n",
        "    print(\" ############ Step = \" + str(s) + \" ############\")\n",
        "    print(\"1) ===BEFORE using any GRADIENT:===\")\n",
        "    #define input and target vectors, and also inform it to the loss function object\n",
        "    #in this code we will keep the same inputs and targets along the steps, but the inputs and\n",
        "    #targets can change when we go, for instance, through the training set\n",
        "    inputs = np.array([[1.2, -1]]) #define the input for the current iteration (step)\n",
        "    outputs = model.predict(inputs) #forward pass\n",
        "    #define the target vector for the current iteration (step)\n",
        "    targets = np.array([[0, 0.9]]) #notice that a sigmoid can output within range [0, 1]\n",
        "    mse = mean_squared_error(targets, outputs) #calculate MSE\n",
        "    #initialize loss object to be later incorporated to the gradients object that\n",
        "    #enables the calculation of the symbolic gradients\n",
        "    loss = losses.mean_squared_error(targets, model.output)\n",
        "    #  ===== Obtain symbolic gradient to calculate numerical gradients =====\n",
        "    gradients = k.gradients(loss, model.trainable_weights) #inform loss and weights\n",
        "    if False: #enable with True in case you want to see the objects\n",
        "        print(\"List of tensors representing the symbolic gradients:\")\n",
        "        for i in range(len(gradients)):\n",
        "            print('symbolic gradient[',i,']=',gradients[i])\n",
        "    print('Network input [x1, x2]:\\n', inputs)\n",
        "    print('Network output [out_o1, out_o2]:\\n', outputs)\n",
        "    print(\"targets:\\n\", targets)\n",
        "    #show weights at the beginning of iteration s\n",
        "    print('weights at the beginning of this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        #note that model.trainable_weights[i] is an object of\n",
        "        # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "        #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "        #you need to obtain its value via a TF session:\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "    print(\"MSE with the initial weights:\", mse)\n",
        "\n",
        "    print(\"2) ===After applying GRADIENT:===\")\n",
        "    print(\"------------- Results for step (iteration) =\", s)\n",
        "    # ===== Calculate numerical gradient from symbolic gradients =====\n",
        "    # evaluated_gradients is a list, show its contents:\n",
        "    evaluated_gradients = sess.run(gradients, feed_dict={model.input: inputs})\n",
        "    print('Gradients g to be used in new_weights = current_weights - learning_rate*g')\n",
        "    for i in range(len(evaluated_gradients)):\n",
        "        print('gradients[',i,']=',evaluated_gradients[i])\n",
        "\n",
        "    # Apply (\"step down\") the gradient for each layer, subtracting the gradients\n",
        "    # from current weights scaled by the learning rate:\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        sess.run(tf.compat.v1.assign_sub(model.trainable_weights[i], learning_rate*evaluated_gradients[i]))\n",
        "\n",
        "    #show weights after gradient propagation of iteration s\n",
        "    print('weights after gradient propagation in this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "\n",
        "    # print the MSE with new weights:\n",
        "    outputs = model.predict(inputs)\n",
        "    mse = mean_squared_error(targets, outputs)\n",
        "    print(\"MSE with the new weights:\", mse)\n",
        "\n",
        "#Collect and show final results\n",
        "final_outputs = model.predict(inputs)\n",
        "final_mse = mean_squared_error(targets, final_outputs)\n",
        "\n",
        "print(\"\\n ===AFTER executing all GRADIENT descent steps===\")\n",
        "#show weights at the end of training\n",
        "for i in range(len(model.trainable_weights)):\n",
        "    #note that model.trainable_weights[i] is an object of\n",
        "    # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "    #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "    #you need to obtain its value via a TF session:\n",
        "    print('final weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "print(\"outputs:\\n\", final_outputs)\n",
        "print(\"targets:\\n\", targets)\n",
        "print(\"Final MSE = \", final_mse)"
      ],
      "metadata": {
        "id": "w1q3wjkL2NGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AMOSTRA 3\n",
        "![AMOSTRA 3.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "uWfZCmKBu97q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras import backend as k\n",
        "from keras import losses\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Initialize all weights to match the values adopted in the example at\n",
        "#https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ \n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2) \n",
        "    # and create a fixed array with this dimension for layer 1\n",
        "    kernel = np.array([[0.1, 0.3],[-0.4, 0.2]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2)\n",
        "    # and create a fixed array with this dimension for layer 2\n",
        "    kernel = np.array([[0.4, -0.4], [0.2, 0.7]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 1\n",
        "    kernel = np.array([-0.4, 0.8]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 2\n",
        "    kernel = np.array([0.80, 0.2]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "# from https://stackoverflow.com/questions/66221788/tf-gradients-is-not-supported-when-eager-execution-is-enabled-use-tf-gradientta\n",
        "# TF 2 does not use \"eager\" execution, so disable it:\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "#Define the neural network model with dense layers. Syntax:\n",
        "#https://keras.io/api/layers/core_layers/dense/\n",
        "#The sigmoid activation function in Keras is the standard logistic function 1/(1+exp(-x)).\n",
        "#https://keras.io/api/layers/activations/\n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=2, use_bias=True,  bias_initializer=kernel_init_bias1,\n",
        "        kernel_initializer=kernel_init_layer1, activation='sigmoid'))\n",
        "model.add(Dense(2, use_bias=True,  bias_initializer=kernel_init_bias2, \n",
        "        kernel_initializer=kernel_init_layer2, activation='sigmoid'))\n",
        "model.summary() # display the architecture\n",
        "\n",
        "# We are not going to use a Keras optimizer. Define a learning rate:\n",
        "learning_rate = 1  #you can change to 0.5 or any other reasonable value\n",
        "\n",
        "#now compile the model, informing loss and performanc metrics that\n",
        "#should be computed along the training\n",
        "model.compile(loss='mse', metrics=['accuracy'])\n",
        "\n",
        "# Begin TensorFlow\n",
        "sess = tf.compat.v1.InteractiveSession()\n",
        "sess.run(tf.compat.v1.initialize_all_variables())\n",
        "\n",
        "print(\"===START TRAINING THE NETWORK:===\")\n",
        "steps = 3 # steps of gradient descent\n",
        "\n",
        "model.optimizer.lr = learning_rate\n",
        "\n",
        "for s in range(steps):\n",
        "    print('Learning rate = ', k.get_value(model.optimizer.lr))\n",
        "    print(\" ############ Step = \" + str(s) + \" ############\")\n",
        "    print(\"1) ===BEFORE using any GRADIENT:===\")\n",
        "    #define input and target vectors, and also inform it to the loss function object\n",
        "    #in this code we will keep the same inputs and targets along the steps, but the inputs and\n",
        "    #targets can change when we go, for instance, through the training set\n",
        "    inputs = np.array([[0.0, 0]]) #define the input for the current iteration (step)\n",
        "    outputs = model.predict(inputs) #forward pass\n",
        "    #define the target vector for the current iteration (step)\n",
        "    targets = np.array([[0, 0.9]]) #notice that a sigmoid can output within range [0, 1]\n",
        "    mse = mean_squared_error(targets, outputs) #calculate MSE\n",
        "    #initialize loss object to be later incorporated to the gradients object that\n",
        "    #enables the calculation of the symbolic gradients\n",
        "    loss = losses.mean_squared_error(targets, model.output)\n",
        "    #  ===== Obtain symbolic gradient to calculate numerical gradients =====\n",
        "    gradients = k.gradients(loss, model.trainable_weights) #inform loss and weights\n",
        "    if False: #enable with True in case you want to see the objects\n",
        "        print(\"List of tensors representing the symbolic gradients:\")\n",
        "        for i in range(len(gradients)):\n",
        "            print('symbolic gradient[',i,']=',gradients[i])\n",
        "    print('Network input [x1, x2]:\\n', inputs)\n",
        "    print('Network output [out_o1, out_o2]:\\n', outputs)\n",
        "    print(\"targets:\\n\", targets)\n",
        "    #show weights at the beginning of iteration s\n",
        "    print('weights at the beginning of this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        #note that model.trainable_weights[i] is an object of\n",
        "        # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "        #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "        #you need to obtain its value via a TF session:\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "    print(\"MSE with the initial weights:\", mse)\n",
        "\n",
        "    print(\"2) ===After applying GRADIENT:===\")\n",
        "    print(\"------------- Results for step (iteration) =\", s)\n",
        "    # ===== Calculate numerical gradient from symbolic gradients =====\n",
        "    # evaluated_gradients is a list, show its contents:\n",
        "    evaluated_gradients = sess.run(gradients, feed_dict={model.input: inputs})\n",
        "    print('Gradients g to be used in new_weights = current_weights - learning_rate*g')\n",
        "    for i in range(len(evaluated_gradients)):\n",
        "        print('gradients[',i,']=',evaluated_gradients[i])\n",
        "\n",
        "    # Apply (\"step down\") the gradient for each layer, subtracting the gradients\n",
        "    # from current weights scaled by the learning rate:\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        sess.run(tf.compat.v1.assign_sub(model.trainable_weights[i], learning_rate*evaluated_gradients[i]))\n",
        "\n",
        "    #show weights after gradient propagation of iteration s\n",
        "    print('weights after gradient propagation in this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "\n",
        "    # print the MSE with new weights:\n",
        "    outputs = model.predict(inputs)\n",
        "    mse = mean_squared_error(targets, outputs)\n",
        "    print(\"MSE with the new weights:\", mse)\n",
        "\n",
        "#Collect and show final results\n",
        "final_outputs = model.predict(inputs)\n",
        "final_mse = mean_squared_error(targets, final_outputs)\n",
        "\n",
        "print(\"\\n ===AFTER executing all GRADIENT descent steps===\")\n",
        "#show weights at the end of training\n",
        "for i in range(len(model.trainable_weights)):\n",
        "    #note that model.trainable_weights[i] is an object of\n",
        "    # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "    #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "    #you need to obtain its value via a TF session:\n",
        "    print('final weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "print(\"outputs:\\n\", final_outputs)\n",
        "print(\"targets:\\n\", targets)\n",
        "print(\"Final MSE = \", final_mse)"
      ],
      "metadata": {
        "id": "Ewqfzi2f5tov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilizando learning rate de **(0.5)** para todas as amostras acima"
      ],
      "metadata": {
        "id": "IofLWkDH-x_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Amostra 1"
      ],
      "metadata": {
        "id": "NeV80KB9_BMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras import backend as k\n",
        "from keras import losses\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Initialize all weights to match the values adopted in the example at\n",
        "#https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ \n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2) \n",
        "    # and create a fixed array with this dimension for layer 1\n",
        "    kernel = np.array([[0.1, 0.3],[-0.4, 0.2]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2)\n",
        "    # and create a fixed array with this dimension for layer 2\n",
        "    kernel = np.array([[0.4, -0.4], [0.2, 0.7]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 1\n",
        "    kernel = np.array([-0.4, 0.8]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 2\n",
        "    kernel = np.array([0.80, 0.2]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "# from https://stackoverflow.com/questions/66221788/tf-gradients-is-not-supported-when-eager-execution-is-enabled-use-tf-gradientta\n",
        "# TF 2 does not use \"eager\" execution, so disable it:\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "#Define the neural network model with dense layers. Syntax:\n",
        "#https://keras.io/api/layers/core_layers/dense/\n",
        "#The sigmoid activation function in Keras is the standard logistic function 1/(1+exp(-x)).\n",
        "#https://keras.io/api/layers/activations/\n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=2, use_bias=True,  bias_initializer=kernel_init_bias1,\n",
        "        kernel_initializer=kernel_init_layer1, activation='sigmoid'))\n",
        "model.add(Dense(2, use_bias=True,  bias_initializer=kernel_init_bias2, \n",
        "        kernel_initializer=kernel_init_layer2, activation='sigmoid'))\n",
        "model.summary() # display the architecture\n",
        "\n",
        "# We are not going to use a Keras optimizer. Define a learning rate:\n",
        "learning_rate = 0.5  #you can change to 0.5 or any other reasonable value\n",
        "\n",
        "#now compile the model, informing loss and performanc metrics that\n",
        "#should be computed along the training\n",
        "model.compile(loss='mse', metrics=['accuracy'])\n",
        "\n",
        "# Begin TensorFlow\n",
        "sess = tf.compat.v1.InteractiveSession()\n",
        "sess.run(tf.compat.v1.initialize_all_variables())\n",
        "\n",
        "print(\"===START TRAINING THE NETWORK:===\")\n",
        "steps = 3 # steps of gradient descent\n",
        "\n",
        "model.optimizer.lr = learning_rate\n",
        "\n",
        "for s in range(steps):\n",
        "    print('Learning rate = ', k.get_value(model.optimizer.lr))\n",
        "    print(\" ############ Step = \" + str(s) + \" ############\")\n",
        "    print(\"1) ===BEFORE using any GRADIENT:===\")\n",
        "    #define input and target vectors, and also inform it to the loss function object\n",
        "    #in this code we will keep the same inputs and targets along the steps, but the inputs and\n",
        "    #targets can change when we go, for instance, through the training set\n",
        "    inputs = np.array([[0.0, -1]]) #define the input for the current iteration (step)\n",
        "    outputs = model.predict(inputs) #forward pass\n",
        "    #define the target vector for the current iteration (step)\n",
        "    targets = np.array([[0, -1.3]]) #notice that a sigmoid can output within range [0, 1]\n",
        "    mse = mean_squared_error(targets, outputs) #calculate MSE\n",
        "    #initialize loss object to be later incorporated to the gradients object that\n",
        "    #enables the calculation of the symbolic gradients\n",
        "    loss = losses.mean_squared_error(targets, model.output)\n",
        "    #  ===== Obtain symbolic gradient to calculate numerical gradients =====\n",
        "    gradients = k.gradients(loss, model.trainable_weights) #inform loss and weights\n",
        "    if False: #enable with True in case you want to see the objects\n",
        "        print(\"List of tensors representing the symbolic gradients:\")\n",
        "        for i in range(len(gradients)):\n",
        "            print('symbolic gradient[',i,']=',gradients[i])\n",
        "    print('Network input [x1, x2]:\\n', inputs)\n",
        "    print('Network output [out_o1, out_o2]:\\n', outputs)\n",
        "    print(\"targets:\\n\", targets)\n",
        "    #show weights at the beginning of iteration s\n",
        "    print('weights at the beginning of this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        #note that model.trainable_weights[i] is an object of\n",
        "        # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "        #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "        #you need to obtain its value via a TF session:\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "    print(\"MSE with the initial weights:\", mse)\n",
        "\n",
        "    print(\"2) ===After applying GRADIENT:===\")\n",
        "    print(\"------------- Results for step (iteration) =\", s)\n",
        "    # ===== Calculate numerical gradient from symbolic gradients =====\n",
        "    # evaluated_gradients is a list, show its contents:\n",
        "    evaluated_gradients = sess.run(gradients, feed_dict={model.input: inputs})\n",
        "    print('Gradients g to be used in new_weights = current_weights - learning_rate*g')\n",
        "    for i in range(len(evaluated_gradients)):\n",
        "        print('gradients[',i,']=',evaluated_gradients[i])\n",
        "\n",
        "    # Apply (\"step down\") the gradient for each layer, subtracting the gradients\n",
        "    # from current weights scaled by the learning rate:\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        sess.run(tf.compat.v1.assign_sub(model.trainable_weights[i], learning_rate*evaluated_gradients[i]))\n",
        "\n",
        "    #show weights after gradient propagation of iteration s\n",
        "    print('weights after gradient propagation in this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "\n",
        "    # print the MSE with new weights:\n",
        "    outputs = model.predict(inputs)\n",
        "    mse = mean_squared_error(targets, outputs)\n",
        "    print(\"MSE with the new weights:\", mse)\n",
        "\n",
        "#Collect and show final results\n",
        "final_outputs = model.predict(inputs)\n",
        "final_mse = mean_squared_error(targets, final_outputs)\n",
        "\n",
        "print(\"\\n ===AFTER executing all GRADIENT descent steps===\")\n",
        "#show weights at the end of training\n",
        "for i in range(len(model.trainable_weights)):\n",
        "    #note that model.trainable_weights[i] is an object of\n",
        "    # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "    #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "    #you need to obtain its value via a TF session:\n",
        "    print('final weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "print(\"outputs:\\n\", final_outputs)\n",
        "print(\"targets:\\n\", targets)\n",
        "print(\"Final MSE = \", final_mse)"
      ],
      "metadata": {
        "id": "EpXZ4umK_CSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Amostra 2"
      ],
      "metadata": {
        "id": "EmWSzDhhDgCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras import backend as k\n",
        "from keras import losses\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Initialize all weights to match the values adopted in the example at\n",
        "#https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ \n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2) \n",
        "    # and create a fixed array with this dimension for layer 1\n",
        "    kernel = np.array([[0.1, 0.3],[-0.4, 0.2]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2)\n",
        "    # and create a fixed array with this dimension for layer 2\n",
        "    kernel = np.array([[0.4, -0.4], [0.2, 0.7]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 1\n",
        "    kernel = np.array([-0.4, 0.8]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 2\n",
        "    kernel = np.array([0.80, 0.2]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "# from https://stackoverflow.com/questions/66221788/tf-gradients-is-not-supported-when-eager-execution-is-enabled-use-tf-gradientta\n",
        "# TF 2 does not use \"eager\" execution, so disable it:\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "#Define the neural network model with dense layers. Syntax:\n",
        "#https://keras.io/api/layers/core_layers/dense/\n",
        "#The sigmoid activation function in Keras is the standard logistic function 1/(1+exp(-x)).\n",
        "#https://keras.io/api/layers/activations/\n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=2, use_bias=True,  bias_initializer=kernel_init_bias1,\n",
        "        kernel_initializer=kernel_init_layer1, activation='sigmoid'))\n",
        "model.add(Dense(2, use_bias=True,  bias_initializer=kernel_init_bias2, \n",
        "        kernel_initializer=kernel_init_layer2, activation='sigmoid'))\n",
        "model.summary() # display the architecture\n",
        "\n",
        "# We are not going to use a Keras optimizer. Define a learning rate:\n",
        "learning_rate = 0.5  #you can change to 0.5 or any other reasonable value\n",
        "\n",
        "#now compile the model, informing loss and performanc metrics that\n",
        "#should be computed along the training\n",
        "model.compile(loss='mse', metrics=['accuracy'])\n",
        "\n",
        "# Begin TensorFlow\n",
        "sess = tf.compat.v1.InteractiveSession()\n",
        "sess.run(tf.compat.v1.initialize_all_variables())\n",
        "\n",
        "print(\"===START TRAINING THE NETWORK:===\")\n",
        "steps = 3 # steps of gradient descent\n",
        "\n",
        "model.optimizer.lr = learning_rate\n",
        "\n",
        "for s in range(steps):\n",
        "    print('Learning rate = ', k.get_value(model.optimizer.lr))\n",
        "    print(\" ############ Step = \" + str(s) + \" ############\")\n",
        "    print(\"1) ===BEFORE using any GRADIENT:===\")\n",
        "    #define input and target vectors, and also inform it to the loss function object\n",
        "    #in this code we will keep the same inputs and targets along the steps, but the inputs and\n",
        "    #targets can change when we go, for instance, through the training set\n",
        "    inputs = np.array([[1.2, -1]]) #define the input for the current iteration (step)\n",
        "    outputs = model.predict(inputs) #forward pass\n",
        "    #define the target vector for the current iteration (step)\n",
        "    targets = np.array([[0, 0.9]]) #notice that a sigmoid can output within range [0, 1]\n",
        "    mse = mean_squared_error(targets, outputs) #calculate MSE\n",
        "    #initialize loss object to be later incorporated to the gradients object that\n",
        "    #enables the calculation of the symbolic gradients\n",
        "    loss = losses.mean_squared_error(targets, model.output)\n",
        "    #  ===== Obtain symbolic gradient to calculate numerical gradients =====\n",
        "    gradients = k.gradients(loss, model.trainable_weights) #inform loss and weights\n",
        "    if False: #enable with True in case you want to see the objects\n",
        "        print(\"List of tensors representing the symbolic gradients:\")\n",
        "        for i in range(len(gradients)):\n",
        "            print('symbolic gradient[',i,']=',gradients[i])\n",
        "    print('Network input [x1, x2]:\\n', inputs)\n",
        "    print('Network output [out_o1, out_o2]:\\n', outputs)\n",
        "    print(\"targets:\\n\", targets)\n",
        "    #show weights at the beginning of iteration s\n",
        "    print('weights at the beginning of this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        #note that model.trainable_weights[i] is an object of\n",
        "        # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "        #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "        #you need to obtain its value via a TF session:\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "    print(\"MSE with the initial weights:\", mse)\n",
        "\n",
        "    print(\"2) ===After applying GRADIENT:===\")\n",
        "    print(\"------------- Results for step (iteration) =\", s)\n",
        "    # ===== Calculate numerical gradient from symbolic gradients =====\n",
        "    # evaluated_gradients is a list, show its contents:\n",
        "    evaluated_gradients = sess.run(gradients, feed_dict={model.input: inputs})\n",
        "    print('Gradients g to be used in new_weights = current_weights - learning_rate*g')\n",
        "    for i in range(len(evaluated_gradients)):\n",
        "        print('gradients[',i,']=',evaluated_gradients[i])\n",
        "\n",
        "    # Apply (\"step down\") the gradient for each layer, subtracting the gradients\n",
        "    # from current weights scaled by the learning rate:\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        sess.run(tf.compat.v1.assign_sub(model.trainable_weights[i], learning_rate*evaluated_gradients[i]))\n",
        "\n",
        "    #show weights after gradient propagation of iteration s\n",
        "    print('weights after gradient propagation in this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "\n",
        "    # print the MSE with new weights:\n",
        "    outputs = model.predict(inputs)\n",
        "    mse = mean_squared_error(targets, outputs)\n",
        "    print(\"MSE with the new weights:\", mse)\n",
        "\n",
        "#Collect and show final results\n",
        "final_outputs = model.predict(inputs)\n",
        "final_mse = mean_squared_error(targets, final_outputs)\n",
        "\n",
        "print(\"\\n ===AFTER executing all GRADIENT descent steps===\")\n",
        "#show weights at the end of training\n",
        "for i in range(len(model.trainable_weights)):\n",
        "    #note that model.trainable_weights[i] is an object of\n",
        "    # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "    #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "    #you need to obtain its value via a TF session:\n",
        "    print('final weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "print(\"outputs:\\n\", final_outputs)\n",
        "print(\"targets:\\n\", targets)\n",
        "print(\"Final MSE = \", final_mse)"
      ],
      "metadata": {
        "id": "ZwUAWies-_kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Amostra 3"
      ],
      "metadata": {
        "id": "JeTe8Qe2Di1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras import backend as k\n",
        "from keras import losses\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Initialize all weights to match the values adopted in the example at\n",
        "#https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ \n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2) \n",
        "    # and create a fixed array with this dimension for layer 1\n",
        "    kernel = np.array([[0.1, 0.3],[-0.4, 0.2]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (weights matrix) to required value\n",
        "def kernel_init_layer2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (2, 2)\n",
        "    # and create a fixed array with this dimension for layer 2\n",
        "    kernel = np.array([[0.4, -0.4], [0.2, 0.7]])\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias1(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 1\n",
        "    kernel = np.array([-0.4, 0.8]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "#Initialise kernel (bias vector) to required value\n",
        "def kernel_init_bias2(shape,dtype='float'):\n",
        "    # In this simple example, assume that the shape is (1, 2)\n",
        "    # and create a fixed array with this dimension for the \n",
        "    # bias vector of layer 2\n",
        "    kernel = np.array([0.80, 0.2]) #both neurons with the same value\n",
        "    return kernel \n",
        "\n",
        "# from https://stackoverflow.com/questions/66221788/tf-gradients-is-not-supported-when-eager-execution-is-enabled-use-tf-gradientta\n",
        "# TF 2 does not use \"eager\" execution, so disable it:\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "#Define the neural network model with dense layers. Syntax:\n",
        "#https://keras.io/api/layers/core_layers/dense/\n",
        "#The sigmoid activation function in Keras is the standard logistic function 1/(1+exp(-x)).\n",
        "#https://keras.io/api/layers/activations/\n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=2, use_bias=True,  bias_initializer=kernel_init_bias1,\n",
        "        kernel_initializer=kernel_init_layer1, activation='sigmoid'))\n",
        "model.add(Dense(2, use_bias=True,  bias_initializer=kernel_init_bias2, \n",
        "        kernel_initializer=kernel_init_layer2, activation='sigmoid'))\n",
        "model.summary() # display the architecture\n",
        "\n",
        "# We are not going to use a Keras optimizer. Define a learning rate:\n",
        "learning_rate = 0.5  #you can change to 0.5 or any other reasonable value\n",
        "\n",
        "#now compile the model, informing loss and performanc metrics that\n",
        "#should be computed along the training\n",
        "model.compile(loss='mse', metrics=['accuracy'])\n",
        "\n",
        "# Begin TensorFlow\n",
        "sess = tf.compat.v1.InteractiveSession()\n",
        "sess.run(tf.compat.v1.initialize_all_variables())\n",
        "\n",
        "print(\"===START TRAINING THE NETWORK:===\")\n",
        "steps = 3 # steps of gradient descent\n",
        "\n",
        "model.optimizer.lr = learning_rate\n",
        "\n",
        "for s in range(steps):\n",
        "    print('Learning rate = ', k.get_value(model.optimizer.lr))\n",
        "    print(\" ############ Step = \" + str(s) + \" ############\")\n",
        "    print(\"1) ===BEFORE using any GRADIENT:===\")\n",
        "    #define input and target vectors, and also inform it to the loss function object\n",
        "    #in this code we will keep the same inputs and targets along the steps, but the inputs and\n",
        "    #targets can change when we go, for instance, through the training set\n",
        "    inputs = np.array([[0.0, 0]]) #define the input for the current iteration (step)\n",
        "    outputs = model.predict(inputs) #forward pass\n",
        "    #define the target vector for the current iteration (step)\n",
        "    targets = np.array([[0, 0.9]]) #notice that a sigmoid can output within range [0, 1]\n",
        "    mse = mean_squared_error(targets, outputs) #calculate MSE\n",
        "    #initialize loss object to be later incorporated to the gradients object that\n",
        "    #enables the calculation of the symbolic gradients\n",
        "    loss = losses.mean_squared_error(targets, model.output)\n",
        "    #  ===== Obtain symbolic gradient to calculate numerical gradients =====\n",
        "    gradients = k.gradients(loss, model.trainable_weights) #inform loss and weights\n",
        "    if False: #enable with True in case you want to see the objects\n",
        "        print(\"List of tensors representing the symbolic gradients:\")\n",
        "        for i in range(len(gradients)):\n",
        "            print('symbolic gradient[',i,']=',gradients[i])\n",
        "    print('Network input [x1, x2]:\\n', inputs)\n",
        "    print('Network output [out_o1, out_o2]:\\n', outputs)\n",
        "    print(\"targets:\\n\", targets)\n",
        "    #show weights at the beginning of iteration s\n",
        "    print('weights at the beginning of this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        #note that model.trainable_weights[i] is an object of\n",
        "        # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "        #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "        #you need to obtain its value via a TF session:\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "    print(\"MSE with the initial weights:\", mse)\n",
        "\n",
        "    print(\"2) ===After applying GRADIENT:===\")\n",
        "    print(\"------------- Results for step (iteration) =\", s)\n",
        "    # ===== Calculate numerical gradient from symbolic gradients =====\n",
        "    # evaluated_gradients is a list, show its contents:\n",
        "    evaluated_gradients = sess.run(gradients, feed_dict={model.input: inputs})\n",
        "    print('Gradients g to be used in new_weights = current_weights - learning_rate*g')\n",
        "    for i in range(len(evaluated_gradients)):\n",
        "        print('gradients[',i,']=',evaluated_gradients[i])\n",
        "\n",
        "    # Apply (\"step down\") the gradient for each layer, subtracting the gradients\n",
        "    # from current weights scaled by the learning rate:\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        sess.run(tf.compat.v1.assign_sub(model.trainable_weights[i], learning_rate*evaluated_gradients[i]))\n",
        "\n",
        "    #show weights after gradient propagation of iteration s\n",
        "    print('weights after gradient propagation in this step')\n",
        "    for i in range(len(model.trainable_weights)):\n",
        "        print('weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "\n",
        "    # print the MSE with new weights:\n",
        "    outputs = model.predict(inputs)\n",
        "    mse = mean_squared_error(targets, outputs)\n",
        "    print(\"MSE with the new weights:\", mse)\n",
        "\n",
        "#Collect and show final results\n",
        "final_outputs = model.predict(inputs)\n",
        "final_mse = mean_squared_error(targets, final_outputs)\n",
        "\n",
        "print(\"\\n ===AFTER executing all GRADIENT descent steps===\")\n",
        "#show weights at the end of training\n",
        "for i in range(len(model.trainable_weights)):\n",
        "    #note that model.trainable_weights[i] is an object of\n",
        "    # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable\n",
        "    #therefore (see e.g. https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable )\n",
        "    #you need to obtain its value via a TF session:\n",
        "    print('final weights[',i,']=',sess.run(model.trainable_weights[i]))\n",
        "print(\"outputs:\\n\", final_outputs)\n",
        "print(\"targets:\\n\", targets)\n",
        "print(\"Final MSE = \", final_mse)"
      ],
      "metadata": {
        "id": "MyPqT5eFFB95"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}